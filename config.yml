kan_blocks:
    n_blocks: 12
    layers_hidden: [768, 32, 32, 768]
    grid_size: 5
    spline_order: 3

gpt:
    vocab_size: 50257
    max_position_embeddings: 1024
    hidden_size: 768
    num_hidden_layers: 12
    num_attention_heads: 1
    n_inner: null
    activation_function: gelu_new
    resid_pdrop: 0.1
    embd_pdrop: 0.1
    attn_pdrop: 0.1
    layer_norm_epsilon: 0.0001
    initializer_range: 0.02
    summary_type: cls_index
    summary_use_proj: true
    summary_activation: null
    summary_proj_to_labels: true
    summary_first_dropout: 0.1
    scale_attn_weights: true
    use_cache: true
    bos_token_id: 50256
    eos_token_id: 50256
    scale_attn_by_inverse_layer_idx: false
    reorder_and_upcast_attn: false

train:
    lr:
        learning_rate_init: 0.01
        learning_rate_final: 0.0001
        start_descending_step: 100
        stop_descending_step: 2000
    pretrain_path: pretrained/gpt2.pt
    batch_size: 32
    world_size: 3
    n_steps: 10000
    test_interval: 100
    checkpoint_interval: 100
    checkpoint_retention: 5

data:
    n_tokens: 1024
    train_path: null
    test_path: null