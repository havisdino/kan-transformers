kan_blocks:
  n_blocks: 13
  width: [768, 4, 768]
  grid: 3
  k: 3

gpt:
  vocab_size: 50257
  max_position_embeddings: 1024
  hidden_size: 768
  num_hidden_layers: 12
  num_attention_heads: 1
  n_inner: null
  activation_function: gelu_new
  resid_pdrop: 0.1
  embd_pdrop: 0.1
  attn_pdrop: 0.1
  layer_norm_epsilon: 0.0001
  initializer_range: 0.02
  summary_type: cls_index
  summary_use_proj: true
  summary_activation: null
  summary_proj_to_labels: true
  summary_first_dropout: 0.1
  scale_attn_weights: true
  use_cache: true
  bos_token_id: 50256
  eos_token_id: 50256
  scale_attn_by_inverse_layer_idx: false
  reorder_and_upcast_attn: false

train:
  pretrain_path: null
  learning_rate: 0.001
  batch_size: 128
  n_steps: 10000

data:
  train_path: null
  test_path: null